## Short-term
- make is.GNN and is.trained methods and write is.xxx.gnn_GNN and is.xxx.list methods
- fitGNNonce should actually be fitGNN.list... zumindest fitGNNonce.list
- export the is.xxx.* methods
- export ‘loadGNN’ ‘saveGNN’
- Use all new methods in all demos => update ./old/2021-01-01_demo

## Medium-term
- can we make a suggestion for a 'verbose = 4' to keras?
- implement and export constructor for superclass gnn_Model (slots model and type)
- implement and export methods rModel, fitModel, save/load
- incorporate S4 methods of 'copula' as S3 methods in gnn (objects, rCopula, fitCopula)

## Long-term
- plot method: what to use as 'low' and 'up' values to evaluate NN at (low,...,low,up,low,...,low)?
- Fix: Paper citations (2019 on arXiv or the real publication if published)
- KL(): If an analytical (differentiable) form exits for K-L divergence
        between copulas, we could propose an interesting extension.
        (Simpler) extensions to consider first: encoder maps to non-diagonal
        covariance matrix, Kendall matrix (which we then transform to covariance
        matrix for use in KL()).

library(gnn)

d <- 2
P <- matrix(0.9, nrow = d, ncol = d)
diag(P) <- 1
A <- t(chol(P))
set.seed(271)
ntrn <- 60000
Z <- matrix(rnorm(ntrn * d), ncol = d)
X <- t(A %*% t(Z)) # d-dimensional equicorrelated normal
U <- apply(abs(X), 2, rank) / (ntrn + 1) # pseudo-observations of |X|

GMMN <- GMMN(c(d, 300, d))
GMMN <- fitGNN(GMMN, data = U, batch.size = 500, n.epoch = 3, verbose = 3)
plot_loss(GMMN)

set.seed(271)
V <- rGNN(GMMN, size = 2000) # sample
plot(V, xlab = expression(V[1]), ylab = expression(V[2]))
