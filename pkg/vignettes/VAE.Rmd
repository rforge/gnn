---
title: Using Variational Autoencoders for Generating Images
author: Marius Hofert, Avinash Prasad
date: '`r Sys.Date()`'
output:
  html_vignette:
    css: style.css
vignette: >
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteIndexEntry{Using Variational Autoencoders for generating images}
  %\VignetteEncoding{UTF-8}
---
## Introduction
In this vignette, we demonstrate the application of variational autoencoders (a type of generative neural network) for the task of generating images. In particular, we work with two standard image datasets, Frey faces and fashion-MNIST. The Frey faces dataset contains 1965 images (with image size $20 \times 28$)  of Brendan Frey with different facial expressions and orientations. While a training sample size of 1965 is relatively low compared to other popular datasets, it was sufficient to train a variational autoencoder and generative realistic images. The fashion-MNIST dataset contain 60000 images (training set) of Zalando's article images (see https://research.zalando.com/welcome/mission/research-projects/fashion-mnist/). This dataset was created to replace the popular MNIST dataset of handwritten digits (often used as a benchmark dataset in the deep learning community). Like the MNIST dataset, the fashion-MNIST dataset consists of $28 \times 28$ grayscale images with each image associated with a label from 10 classes. This dataset was essentially created to provided a more challenging version of the MNIST dataset (while keeping the image size, number of classes, and training dataset size the same).

Note that the neural network were pre-trained on a GPU server and we use the generator component of the trained VAEs in this vignette.

We start by loading the R packages we need.


```{r setup, message = FALSE}
## Packages
library(keras) # R interface to Keras (high-level neural network API)
library(tensorflow) # R interface to TensorFlow (numerical computation with tensors)
library(gnn) # for VAE model setup  and saved VAE datasets
library(RnavGraphImageData) # for Frey faces dataset
```

## 2 Frey faces

To begin with, we load the Frey faces training dataset.

```{r 2_definition_training_dataset}
data(frey)
x.frey <- t(frey)/255 # We adjust the dataset such that each row of x.frey  represents a sample and
#normalize the dataset (which contains pixel values in [0,255]) to [0,1] to help train the VAE.
ntrn<-nrow(x.frey) ## Training dataset size
dim.in=dim.out<-ncol(x.frey) ## dimension of training dataset
```

Let us now visualize some images from the training dataset to get a picture of our target distribution for the variational autoencoder.

```{r 2_plot, fig.align = "center", fig.width = 12, fig.height = 12, fig.show = "hold"}
n.frey.disp<-25  ## Number of frey faces to display
dim.frey.row<-20 ## Dimension of a single frey face (row)
dim.frey.col<-28  ## Dimension of a single frey face (column)
# TODO: How to get rid of the white space?
## Plot the 25 images in a 5*5 grid.
par(mfrow=c(5,5),mar=c(0,0,0,0),oma = c(0, 0, 0, 0))
for (i in 1:n.frey.disp){
  plot(as.raster(t(matrix(x.frey[i,],nrow=dim.frey.row))))
}
```

Next, we set up the VAE and train it based on the training dataset `x.frey`. The generator component of the
VAE is saved (or loaded using data() from the gnn package) to be used later for generating Frey faces.

```{r 2_training}
## Training setup
nepo<-300 # epochs (= passes through training dataset while updating NN parameters)
nbat<-100# training batch size (number of samples per stochastic gradient step)
dim.hidden<-300 # dimension of single hidden layer
dim.lat<-2 # dimension of latent layer
VAE.frey<-VAE_model(dim=c(dim.in,dim.hidden,dim.lat)) ## setup the VAE
objname <- paste0("VAE_gen_dim_",dim.lat,"_",dim.hidden,"_",dim.out,"_ntrn_",ntrn,
                  "_nbat_",nbat,"_nepo_",nepo,"_freyfaces")

if(dataset_exists(objname)) { # get trained VAE generator
  VAE.frey$generator<-unserialize_model(read_dataset(objname))
}else {  # train the VAE and save the corresponding generator.
VAE.frey$model %>% fit(x.frey, x.frey, epochs = nepo, batch_size=nbat)
  save_rda(serialize_model(VAE.frey$generator), name = objname, file = paste0(objname,".rda"))
}
```

After training of the VAE, we can use it to generate Frey faces. That is, we feed the learned generator component of the VAE  with $\mathrm{N}(0,1)$ pseudo-random numbers (representing the latent layer). We then visualize the generated images.

```{r 2_sampling_and_visualize, fig.align = "center", fig.width = 12, fig.height = 12, fig.show = "hold"}
set.seed(271) # for reproducibility
ngen.frey.disp<-25  # Number of generator images to display
## Sample from the latent distribution
N01.latent <- matrix(rnorm(ngen.frey.disp * dim.lat), ncol = dim.lat)
# Generate Frey faces from the fitted VAE
x.frey.VAE<-predict(VAE.frey$generator,N01.latent)
# TODO: How to get rid of the white space?
## Plot the 25 images in a 5*5 grid.
par(mfrow=c(5,5),mar=c(0,0,0,0),oma = c(0, 0, 0, 0))
for (i in 1:ngen.frey.disp){
  plot(as.raster(t(matrix(x.frey.VAE[i,],nrow=dim.frey.row))))
}
```

## 3 Fashion-MNIST

To begin with, we load the fashion-MNIST training dataset.


```{r 3_definition_training_dataset}
fmnist<-dataset_fashion_mnist() ## Load the full fashion MNIST dataset from Keras R package
x.fmnist<-fmnist$train$x/255 ## Grab the training dataset of 60000 images and normalize the
#data to [0,1] as done before.
x.fmnist <- t(apply(x.fmnist,1,as.numeric))  # Minor adjustment to create the appropriate traning dataset
ntrn<-nrow(x.fmnist) ## Training dataset size
dim.in=dim.out<-ncol(x.fmnist) ## dimension of training dataset
```

Let us now visualize some images from the training dataset to get a picture of our target distribution for the variational autoencoder.


```{r 3_plot, fig.align = "center", fig.width = 12, fig.height = 12, fig.show = "hold"}
n.fmnist.disp<-25  ## Number of fashion-MNIST images to display
dim.fmnist.row<-28 ## Dimension of a single fashion-MNIST image (row)
dim.fmnist.col<-28  ## Dimension of a single fashion-MNIST image (column)
## Plot the 25 images in a 5*5 grid.
par(mfrow=c(5,5),mar=c(0,0,0,0),oma = c(0, 0, 0, 0))
for (i in 1:n.fmnist.disp){
  plot(as.raster(matrix(x.fmnist[i,],nrow=dim.fmnist.row)))
}
```

Next, we set up the VAE and train it based on the training dataset `x.fmnist`. Since we are working with a higher-dimensional and more complex dataset (greated variety of images when compared to Frey faces), we choose to work with a VAE with three hidden layers instead of one.

```{r 3_training}
## Training setup
nepo<-100 # epochs (= passes through training dataset while updating NN parameters)
nbat<-100# training batch size (number of samples per stochastic gradient step)
dim.hidden<-rep(300,3) # dimensions of the three hidden layers
dim.lat<-2 # dimension of latent layer
VAE.fmnist<-VAE_model(dim=c(dim.in,dim.hidden,dim.lat)) ## setup the VAE
objname <- paste0("VAE_gen_dim_",dim.lat,"_",length(dim.hidden),"_",dim.hidden[1],"_",dim.out,"_ntrn_",ntrn,
                  "_nbat_",nbat,"_nepo_",nepo,"_fmnist")

if(dataset_exists(objname)) { # get trained VAE generator
  VAE.fmnist$generator<-unserialize_model(read_dataset(objname))
}else {  # train the VAE and save the corresponding generator.
VAE.fmnist$model %>% fit(x.fmnist, x.fmnist, epochs = nepo, batch_size=nbat)
  save_rda(serialize_model(VAE.fmnist$generator), name = objname, file = paste0(objname,".rda"))
}
```

After training of the VAE, we can use it to generate fashion-MNIST images. That is, we feed the learned generator component of the VAE with $\mathrm{N}(0,1)$ pseudo-random numbers (representing the latent layer). We then visualize the generated images.

```{r 3_sampling_and_visualize, fig.align = "center", fig.width = 12, fig.height = 12, fig.show = "hold"}
ngen.fmnist.disp<-25  # Number of generator images to display
## Sample from the latent distribution
N01.latent <- matrix(rnorm(ngen.fmnist.disp * dim.lat), ncol = dim.lat)
# Generate Frey faces from the fitted VAE
x.fmnist.VAE<-predict(VAE.fmnist$generator,N01.latent)
## Plot the 25 images in a 5*5 grid.
par(mfrow=c(5,5),mar=c(0,0,0,0),oma = c(0, 0, 0, 0))
for (i in 1:ngen.fmnist.disp){
  plot(as.raster(matrix(x.fmnist.VAE[i,],nrow=dim.fmnist.row)))
}
```

